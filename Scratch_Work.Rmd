---
title: "Scratch_Work"
author: "Matt Johnson"
date: "8/25/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(gridExtra)
```

```{r}
combine <- read.csv("source_data/combine.csv")
draft <- read.csv("source_data/draft.csv")
```

```{r}
head(combine)
str(combine)
summary(combine)
```

```{r}
head(draft)
str(draft)
summary(draft)
```


Initial cleaning
```{r}
#I'm thinking i'm going to use only data from 2000 on. I think these observations will be more complete. Also since the game has changed so much, this sort of analysis is only really relevant in the modern era of football. I also do not need a lot of these variables, lets select important ones 
Comb.2000 <- combine %>% 
  filter(combineYear >= 2000) %>% 
  select(1:6, 8:18, 28:33)
Draft.2000 <- draft %>% 
  filter(draft >= 2000) %>% 
  select(1:19)
  
#ok lets take out any rows with an NA just to see
Comb.compl <- na.omit(Comb.2000) 
Draft.compl <- na.omit(Draft.2000)[, c(1, 3, 4)]
```

Join and make final single csv

```{r}
#join
DF.clean <- na.omit(left_join(Comb.compl, Draft.compl, by = "playerId")[, -c(1, 3, 4:8, 11:13, 16)]) 
  
DF.clean$position <- factor(DF.clean$position)

write.csv(DF.clean, "STOR-565-P1-Data.csv")
```

I think I need to split into positions to really see what's up
```{r}
#Lets fist split into offense and defense and then split more from there
DF.Off.skill <- DF.clean %>% 
  filter(position == "WR" | position == "RB" | position == "TE")
DF.Off.strength <- DF.clean %>% 
  filter(position == "C" | position == "OG" | position == "OT" | position == "OL")
DF.Def.skill <- DF.clean %>% 
  filter(position == "DB" | position == "S")
DF.Def.strength <- DF.clean %>% 
  filter(position == "DE" | position == "DL" | position == "DT" | position == "LB" | position == "OLB")
```

Quick read in cleaned data 
--------------------------
```{r}
DF.Off.skill <- read.csv("derived_data/Off.Skill.csv")
DF.Off.strength <- read.csv("derived_data/Off.Strength.csv")
DF.Def.skill <- read.csv("derived_data/Def.Skill.csv")
DF.Def.strength <- read.csv("derived_data/Def.Strength.csv")
```

Graphs
```{r}
g1 <- ggplot(DF.Off.skill, mapping = aes(x = combine40yd, y = pick, color = position)) +     geom_point(alpha = .5) +
  xlab("40 Time") +
  ylab("Pick") +
  labs(color = "Position", title = "Offense") +
  xlim(4, 5.2) +
  theme(legend.position = c(0, 1),
                  legend.justification = c(0, 1))

g2 <- ggplot(DF.Off.strength, mapping = aes(x = combine40yd, y = pick, color = position)) +     geom_point(alpha = .5) +
  xlab("40 Time") +
  ylab("Pick") +
  labs(color = "Position")

g3 <- ggplot(DF.Def.skill, mapping = aes(x = combine40yd, y = pick, color = position)) +     geom_point(alpha = .5) +
  xlab("40 Time") +
  ylab("Pick") +
  labs(color = "Position", title = "Defense") +
  xlim(4, 5.2) +
  theme(legend.position = c(0, 1),
                  legend.justification = c(0, 1))

g4 <- ggplot(DF.Def.strength, mapping = aes(x = combine40yd, y = pick, color = position)) +     geom_point(alpha = .5) +
  xlab("40 Time") +
  ylab("Pick") +
  labs(color = "Position")

g1
g2
g3
g4
```

```{r}
Graph1 <- grid.arrange(g1, g3, nrow=1)
ggsave("derived_graphs/Off.Def.40s.png", plot = Graph1)
```

weights
```{r}
g5 <- ggplot(DF.Off.skill, aes(weight)) +
  geom_density(aes(color=position), alpha=.5) +
  xlim(170, 400) +
  xlab("Weights") +
  ylab("") +
  labs(color="Position") +
   theme(legend.position = c(.8, 1),
                  legend.justification = c(0, 1))

g6 <- ggplot(DF.Off.strength, aes(weight)) +
  geom_density(aes(color=position), alpha=.5) +
  xlim(170,400) +
  xlab("Weights") +
  ylab("") +
  labs(color="Position") +
   theme(legend.position = c(0, 1),
                  legend.justification = c(0, 1))
g5.2 <- ggplot(DF.Def.skill, aes(weight)) +
  geom_density(aes(color=position), alpha=.5) +
  xlim(170, 400) +
  xlab("Weights") +
  ylab("") +
  labs(color="Position") +
   theme(legend.position = c(.8, 1),
                  legend.justification = c(0, 1))

g6.2 <- ggplot(DF.Def.strength, aes(weight)) +
  geom_density(aes(color=position), alpha=.5) +
  xlim(170,400) +
  xlab("Weights") +
  ylab("") +
  labs(color="Position") +
   theme(legend.position = c(0, 1),
                  legend.justification = c(0, 1))
# g5
# g6
# g5.2
# g6.2
Graph2 <- grid.arrange(g5, g6, g5.2, g6.2, nrow=2, top = "Weight Distribution by Position")
```

```{r}
g7 <- ggplot(DF.Off.skill, aes(ageAtDraft)) +
  geom_histogram(aes(fill = position), position = "dodge") +
  xlab("Age at Draft") +
  ylab("Count") +
  labs(fill = "Position", title = "Offense Skill") +
  theme(legend.position = c(0, 1),
        legend.justification = c(0, 1),
        legend.key.size = unit(.25, "cm")) +
  theme(axis.text.y=element_blank())

g8 <- ggplot(DF.Off.strength, aes(ageAtDraft)) +
  geom_histogram(aes(fill = position), position = "dodge") +
  xlab("Age at Draft") +
  ylab("Count") +
  labs(fill = "Position", title = "Offense Line") +
  theme(legend.position = c(0, 1),
        legend.justification = c(0, 1),
        legend.key.size = unit(.25, "cm")) +
  theme(axis.text.y=element_blank())

g9 <- ggplot(DF.Def.skill, aes(ageAtDraft)) +
  geom_histogram(aes(fill = position), position = "dodge") +
  xlab("Age at Draft") +
  ylab("Count") +
  labs(fill = "Position", title = "Defense Skill") +
  theme(legend.position = c(0, 1),
        legend.justification = c(0, 1),
        legend.key.size = unit(.25, "cm")) +
  theme(axis.text.y=element_blank())

g10 <- ggplot(DF.Def.strength, aes(ageAtDraft)) +
  geom_histogram(aes(fill = position), position = "dodge") +
  xlab("Age at Draft") +
  ylab("Count") +
  labs(fill = "Position", title = "Defense Line") +
  theme(legend.position = c(0, 1),
        legend.justification = c(0, 1),
        legend.key.size = unit(.25, "cm")) +
  theme(axis.text.y=element_blank())

# g7
# g8
# g9
# g10

Graphs3 <- grid.arrange(g7, g8, g9, g10, nrow=2, top = "Age Distribution by Position")
```

```{r}
g11 <- DF.clean %>% 
  filter(position != "QB" & position != "LS") %>% 
  ggplot(aes(x=position, y=round, color = position)) +     
  geom_boxplot() +
  xlab("Position") +
  ylab("Pick") + 
  theme(legend.position = "none")

g11

```

```{r}
g12 <- DF.clean %>% 
  filter(position != "QB" & position != "LS") %>% 
  ggplot(aes(x=pick)) +
  geom_density() +
  xlab("Pick") +
  ylab("Density")
g12
```





Read in full set
```{r}
Clean_Data <- read.csv("derived_data/Clean_Data.csv") %>% 
  mutate(position = as.numeric(position))
```

Add a tag of skill or strength for clustering Combine the 4 data sets with the tag
```{r}
DF.Off.skill <- read.csv("derived_data/Off.Skill.csv") %>% 
  mutate(Type = "1") #skill is 1, strength is 2, linebacker is 3
DF.Off.strength <- read.csv("derived_data/Off.Strength.csv") %>% 
  mutate(Type = "2")
DF.Def.skill <- read.csv("derived_data/Def.Skill.csv" )%>% 
  mutate(Type = "1")
DF.Def.strength <- read.csv("derived_data/Def.Strength.csv" )%>% 
  mutate(Type = "2")
DF.Mixed <- read.csv("derived_data/Df.Mix.csv") %>% 
  mutate(Type = "3")
Skill.Stren.DF <- rbind(DF.Off.skill, DF.Off.strength, DF.Def.strength, DF.Def.skill, DF.Mixed)
```

```{r}
g13 <- Skill.Stren.DF %>% 
  ggplot(aes(x=Type, y=combine40yd, color = Type)) +
  geom_boxplot() +
  scale_color_discrete(name="Type", labels = c("Skill", "Strength", "Mixed"))
g13
```


Initial split of data into test/train/validate
```{r}
set.seed <- 18 #this is my lucky number 
spec = c(train = .6, test = .2, validate = .2)
DF = sample(cut(
  seq(nrow(Clean_Data)), 
  nrow(Clean_Data)*cumsum(c(0,spec)),
  labels = names(spec)
))

Split.DF = split(Clean_Data, DF)
```

Clustering
------
TSNE
```{r}
set.seed = 18
library(Rtsne)
fit1 <- Rtsne(Skill.Stren.DF %>% select(heightInches, weight, 7:12), dims=2, check_duplicates = F)
ggplot(fit1$Y %>% as.data.frame() %>% as_tibble(), aes(V1,V2)) +
    geom_point(aes(color=Skill.Stren.DF$Type)) +
    scale_color_discrete(name="Type", labels = c("Skill", "Strength", "Mixed"))
```
Did not exactly trust TSNE becasue it is non-linear so lets try k-means. 



K-nearest neighbor
```{r}
library(class)
results1 <- do.call(rbind, Map(function(n) {
  predictions <- knn(train = Split.DF$train,
                     test = Split.DF$test, 
                     cl = Split.DF$train$position, 
                     k=n)
  acc <- sum(predictions == Split.DF$validate$position)/length(predictions)
  tibble(k=n, acc=acc)
}, seq(1, 100))) %>% 
  arrange(desc(acc), k)

results1 %>% arrange(desc(acc),k) %>% head(5)
```

K-means

```{r}
set.seed = 18
cc <- kmeans(Skill.Stren.DF %>% select(heightInches, weight, 7:12), 3)
fit1 <- Rtsne(Skill.Stren.DF %>% select(heightInches, weight, 7:12), dims=2, check_duplicates = F)
ggplot(fit1$Y %>% as.data.frame() %>% as_tibble() %>% mutate(label=cc$cluster),aes(V1,V2)) +
    geom_point(aes(color=factor(label))) +
    scale_color_discrete(name="Type", labels = c("Skill", "Strength", "Mixed"))
```

The groups of athletes, although there is sort of a spectrum. Clustering is happening in unreduced number of dimensions and clusters still show up when we reduce the dimensionality. looks like there really are three groups, lets try to see use Principle Component Analysis

Principle Component anlsysis
```{r}
library(ggfortify)
library(cluster)
pca <- prcomp(Skill.Stren.DF %>% select(heightInches, weight, 7:12), scale. = T)
summary(pcs)
pcs <- as.data.frame(pca$x)
pca.data <- cbind(Skill.Stren.DF$pick, pcs)
lm.pca <- lm(Skill.Stren.DF$pic ~ PC1 + 
               PC2 + 
               PC3 +
               PC4 +
               PC5 +
               PC6 +
               PC7 +
               PC8, data = pca.data)
summary(lm.pca)
lm.pca2 <- step(lm.pca, trace = 0)
summary(lm.pca2)
bc.pca <- boxCox(lm.pca2)
opt.lam.pca <- bc.pca$x[which.max(bc.pca$y)]
lm.pca3 <- lm((Skill.Stren.DF$pick)^opt.lam.pca ~ PC1 + PC2 + PC4 + PC5 + PC7 + PC8, data = pca.data)
summary(lm.pca3)
lm.pca4 <- step(lm.pca3, trace = 0)
summary(lm.pca4)
lm.pca5 <- lm((Skill.Stren.DF$pick)^opt.lam.pca ~ PC1 + PC2 + PC5 + PC8, data = pca.data)
summary(lm.pca5)


glm.pca <- glm(Skill.Stren.DF$pic ~ PC1 + 
               PC2 + 
               PC3 +
               PC4 +
               PC5 +
               PC6 +
               PC7 +
               PC8, data = pca.data, family = "poisson")
summary(glm.pca)
glm2.pca <- step(glm.pca, trace = 0)
anova(glm2.pca, glm.pca)



plot(pca,type = "l", main ="Screen plot for PCA")
autoplot(pca, loadings=T, loadings.colour = 'blue', loadings.label.size = 6) #plots eigenvectors
# autoplot(kmeans(Skill.Stren.DF %>% select(heightInches, weight, 7:12), 3), data = Skill.Stren.DF %>% select(heightInches, weight, 7:12))

autoplot(pam(Skill.Stren.DF %>% select(heightInches, weight, 7:12), 3), frame = T, fram.type = 'norm') +
  scale_color_continuous(type = Skill.Stren.DF$pick)+
  guides(fill = F)

autoplot(silhouette(pam(Skill.Stren.DF %>% select(heightInches, weight, 7:12), 3)))

DF <- cbind(Skill.Stren.DF, pcs)

ggplot(DF, aes(x=PC1, y=PC2)) +
         geom_point(aes(color = pick), alpha = .5)
```

Principle component regressions
```{r}
library("pls")
set.seed = 18
pcr.mod <- pcr(pick ~ heightInches + 
              weight + 
              ageAtDraft + 
              combine40yd + 
              combineVert + 
              combineBench +
              combineShuttle +
              combineBroad +
              combine3cone, data = Split.DF$train, scale = T, validation = "CV")
summary(pcr.mod)

validationplot(pcr.mod, val.type = "R2")

predictions.pcr <- predict(pcr.mod, newdata = Split.DF$test, ncomp = 3)
SSE.pcr <- sum((predictions.pcr - Split.DF$test$pick)^2)
RMSE.pcr <- sqrt(SSE.pcr/nrow(Split.DF$test))
RMSE.pcr
```





Models
------
LM
```{r}
lm1 <- lm(pick ~ heightInches + 
              weight + 
              ageAtDraft + 
              combine40yd + 
              combineVert + 
              combineBench +
              combineShuttle +
              combineBroad +
              combine3cone, data = Split.DF$train)
summary(lm1)
lm2 <- step(lm1, trace = 0)
summary(lm2)
anova(lm1, lm2)
plot(lm2)

library(car)
bc <- boxCox(lm1)
lam <- bc$x[which.max(bc$y)]
lm3 <- lm((pick)^lam ~ heightInches + 
              weight + 
              ageAtDraft + 
              combine40yd + 
              combineVert + 
              combineBench +
              combineShuttle +
              combineBroad +
              combine3cone, data = Split.DF$train)
summary(lm3)

lm4 <- step(lm3, trace = 0)
summary(lm4)

anova(lm3, lm4)

predictions.lm4 <- predict(lm4, Split.DF$test[, -c(1, 2, 3, 13, 14)], type = "response")
SSE.lm4 <- sum((predictions.lm4 - Split.DF$test$pick)^2)
RMSE.lm4 <- sqrt(SSE.lm4/nrow(Split.DF$test))
RMSE.lm4

vif(lm4)

lm5 <- lm((pick)^lam ~ ageAtDraft + 
            combine40yd + 
            combineBench + 
            combineBroad + 
            combine3cone, data = Split.DF$train)

predictions.lm5 <- predict(lm5, Split.DF$test[, -c(1, 2, 3, 13, 14)], type = "response")
SSE.lm5 <- sum((predictions.lm5 - Split.DF$test$pick)^2)
RMSE.lm5 <- sqrt(SSE.lm5/nrow(Split.DF$test))
RMSE.lm5
vif(lm5)
```

GLM
```{r}
glm1 <- glm(pick ~ heightInches + 
              weight + 
              ageAtDraft + 
              combine40yd + 
              combineVert + 
              combineBench +
              combineShuttle +
              combineBroad +
              combine3cone, family = poisson, data = Split.DF$train)
anova(glm1, test = "Chisq")


glm2 <- step(glm1, direction = "both", trace = 0)
summary(glm2)
anova(glm2, test = "Chisq")

predictions.glm1 <- predict(glm1, Split.DF$test[, -c(1, 2, 3, 13, 14)])
SSE.glm1 <- sum((predictions.glm1 - Split.DF$test$pick)^2)
RMSE.glm1 <- sqrt(SSE.glm1/nrow(Split.DF$test))
RMSE.glm1


predictions.glm2 <- predict(glm2, Split.DF$test[, -c(1, 2, 3, 13, 14)])
SSE.glm2 <- sum((predictions.glm2 - Split.DF$test$pick)^2)
RMSE.glm2 <- sqrt(SSE.glm2/nrow(Split.DF$test))
RMSE.glm2

```

Lasso and Ridge
```{r}
set.seed = 18
library(glmnet)

x <- as.matrix(Split.DF$train[,-c(1, 2, 3, 13, 14)])
y_train <- Split.DF$train$pick

x_test <- as.matrix(Split.DF$test[,-c(1, 2, 3, 13, 14)])
y_test <- Split.DF$test$pick

lambdas <- 10^seq(2, -3, by = -.1)

ridge.reg <- glmnet(x, y_train, nlambda = 25, alpha = 0, family = 'poisson', lambda = lambdas)

summary(ridge.reg)
plot(ridge.reg)

lasso.reg <- glmnet(x, y_train, nlambda = 25, alpha = 1, family = 'poisson', lambda = lambdas)

summary(lasso.reg)
plot(lasso.reg)

cv_ridge <- cv.glmnet(x, y_train, alpha = 0, lambda = lambdas)
optimal_lambda.ridge <- cv_ridge$lambda.min
optimal_lambda.ridge

cv_lasso <- cv.glmnet(x, y_train, alpha = 1, lambda = lambdas)
optimal_lambda.lasso <- cv_ridge$lambda.min
optimal_lambda.lasso


# Compute R^2 from true and predicted values
eval_results <- function(true, predicted, df) {
  SSE <- sum((predicted - true)^2)
  SST <- sum((true - mean(true))^2)
  R_square <- 1 - SSE / SST
  RMSE = sqrt(SSE/nrow(df))

  
# Model performance metrics
data.frame(
  RMSE = RMSE,
  Rsquare = R_square
)
  
}

lasso_model <- glmnet(x, y_train, alpha = 1, lambda = optimal_lambda.lasso, standardize = TRUE)

predictions_train <- predict(lasso_model, s = optimal_lambda.lasso, newx = x)
eval_results(y_train, predictions_train, Split.DF$train)

predictions_test <- predict(lasso_model, s = optimal_lambda.lasso, newx = x_test)
eval_results(y_test, predictions_test, Split.DF$test)

predictions_validate <- predict(lasso_model, s = optimal_lambda.lasso, newx = as.matrix(Split.DF$validate[,-c(1, 2, 3, 13, 14)]))
eval_results(Split.DF$validate$pick, predictions_validate, Split.DF$validate)



ridge_model <- glmnet(x, y_train, alpha = 0, lambda = optimal_lambda.ridge, standardize = TRUE)

predictions_train <- predict(ridge_model, s = optimal_lambda.ridge, newx = x)
eval_results(y_train, predictions_train, Split.DF$train)

predictions_test <- predict(ridge_model, s = optimal_lambda.ridge, newx = x_test)
eval_results(y_test, predictions_test, Split.DF$test)

predictions_validate <- predict(ridge_model, s = optimal_lambda.ridge, newx = as.matrix(Split.DF$validate[,-c(1, 2, 3, 13, 14)]))
eval_results(Split.DF$validate$pick, predictions_validate, Split.DF$validate)
```

Random Forest
```{r}
library(randomForest)

oob.err=double(9)
test.err=double(9)

for (mtry in 1:9) {

rf1 <- randomForest(pick ~ heightInches + 
                      weight + 
                      ageAtDraft +
                      combineShuttle + 
                      combineBroad + 
                      combine3cone + 
                      combineBench + 
                      combine40yd + 
                      combineVert, 
                    data = Split.DF$train[, -c(1, 2, 3, 13)], mtry = mtry, ntree=500,
                    xtest = Split.DF$test[, -c(1, 2, 3, 13, 14)],
                    yest = Split.DF$test$pick, keep.forest = T)
oob.err[mtry] = rf1$mse[500] #error of all trees fitted

pred <- predict(rf1, Split.DF$test[, -c(1, 2, 3, 13, 14)], type = "response")
test.err[mtry] = with(Split.DF$test[, -c(1, 2, 3, 13)], mean((pick - pred)^2))

cat(mtry, " ")
}

which.min(test.err)


rf2 <- randomForest(pick ~ heightInches + 
                      weight + 
                      ageAtDraft +
                      combineShuttle + 
                      combineBroad + 
                      combine3cone + 
                      combineBench + 
                      combine40yd + 
                      combineVert, 
                    data = Split.DF$train[, -c(1, 2, 3, 13)], type = "regression", mtry = 4, ntree=500, 
                    xtest = Split.DF$test[, -c(1, 2, 3, 13, 14)], 
                    ytest = Split.DF$test$pick, keep.forest = T)

predictions.rf2 <- predict(rf2, Split.DF$test[, -c(1, 2, 3, 13, 14)])

SSE.rf2 <- sum((predictions.rf2 - Split.DF$test$pick)^2)
RMSE.rf2 <- sqrt(SSE.rf2/nrow(Split.DF$test))
RMSE.rf2


varImpPlot(rf2)
importance(rf2)


rf3 <- rf2 <- randomForest(pick ~ heightInches + 
                      weight + 
                      ageAtDraft +
                      combineShuttle + 
                      combineBroad + 
                      combine3cone + 
                      combineBench + 
                      combine40yd + 
                      combineVert, 
                    data = Split.DF$train[, -c(1, 2, 3, 13)], type = "regression", mtry = 4, ntree=500, 
                    xtest = Split.DF$test[, -c(1, 2, 3, 13, 14)], 
                    ytest = Split.DF$test$pick, keep.forest = T)
```

Gradient booster
```{r}
library(gbm)
gbm1 <-gbm(pick ~ heightInches + 
                      weight + 
                      ageAtDraft +
                      combineShuttle + 
                      combineBroad + 
                      combine3cone + 
                      combineBench + 
                      combine40yd + 
                      combineVert, 
           distribution = "gaussian", data = Split.DF$train[, -c(1, 2, 3, 13)], interaction.depth = 3, n.trees = 500)

summary(gbm1, plot = T)
predictions.gbm <- predict(gbm1, Split.DF$train[, -c(1, 2, 3, 13, 14)])
SSE.gbm <- sum((predictions.gbm - Split.DF$train$pick)^2)
RMSE.gbm <- sqrt(SSE.gbm/nrow(Split.DF$train))
RMSE.gbm


predictions.gbm <- predict(gbm1, Split.DF$test[, -c(1, 2, 3, 13, 14)])
SSE.gbm <- sum((predictions.gbm - Split.DF$test$pick)^2)
RMSE.gbm <- sqrt(SSE.gbm/nrow(Split.DF$test))
RMSE.gbm
```

